{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a97e9f4-9039-4b39-b466-02bc8d9353fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from lexrank import STOPWORDS, LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa106f9c-bca7-4983-a8a0-7ab4cb83c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions:\n",
    "def to_paragraph(text):\n",
    "    \"\"\"\n",
    "    converts text comprised of lists of sentances to a single to_paragraph\n",
    "\n",
    "    text - list of multiple string sentances\n",
    "\n",
    "    continuous_text - single continuous text string\n",
    "    \"\"\"\n",
    "    continuous_text = ''\n",
    "    for line in text:\n",
    "        continuous_text += line\n",
    "    return continuous_text\n",
    "\n",
    "def shuffle_and_sample(data):\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "    return data[:600]\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path) as f:\n",
    "        contents = f.readlines()\n",
    "    return contents\n",
    "\n",
    "def parse_paper(json_paper):\n",
    "    first_section = to_paragraph(json_paper['sections'][0])\n",
    "    last_section = to_paragraph(json_paper['sections'][-1])\n",
    "    first_n_last = first_section + last_section\n",
    "    return first_n_last\n",
    "\n",
    "def to_json(str_blob):\n",
    "    json_paper = json.loads(str_blob)\n",
    "    return json_paper\n",
    "\n",
    "def to_paragraph(text):\n",
    "    \"\"\"\n",
    "    converts text comprised of lists of sentances to a single to_paragraph\n",
    "\n",
    "    text - list of multiple string sentances\n",
    "\n",
    "    continuous_text - single continuous text string\n",
    "    \"\"\"\n",
    "    continuous_text = ''\n",
    "    for line in text:\n",
    "        continuous_text += line\n",
    "    return continuous_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a776f27-c688-4de8-89c1-88ccb4a1465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_data/val.txt') as f:\n",
    "    val = f.readlines()\n",
    "# creates json list of validation documents\n",
    "val_json_list = []\n",
    "for i in val:\n",
    "    val_json_list.append(to_json(i[:-1]))\n",
    "val_json_dict = {\"data\": val_json_list}\n",
    "\n",
    "with open('json_val.txt', 'w') as outfile:\n",
    "    json.dump(val_json_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "330b7330-2f59-499b-987e-fffa422adc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open mini-val data as a list of dicts\n",
    "with open('json_val.txt') as json_file:\n",
    "    val = json.load(json_file)\n",
    "val = val['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "379e0f72-7aff-43c5-a81b-c3ca7424ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_order_temporal(data):\n",
    "    #print('hi 1')\n",
    "    \"\"\"\n",
    "    data - input data in json format with attribute \"article_text\" and \"abstract_text\"\n",
    "    \n",
    "    re_ordered_df - dataframe of schema: text | target, where text is the truncated and temporally corrected text, and the target is the raw abstract\n",
    "    \n",
    "    \"\"\"\n",
    "    documents = [i['article_text'] for i in data]\n",
    "    truncated_df = pd.DataFrame(columns=['text', 'target'])\n",
    "    # instantiate tokenizer and lexranker\n",
    "    lxr = LexRank(documents[:1000], stopwords=STOPWORDS['en'])\n",
    "    #iterate over dataset and assemble dataframe\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        summary = to_paragraph(i['abstract_text'])\n",
    "        # create data frame of schema sentance | rank | order \n",
    "        rank = lxr.rank_sentences(sentences = i['article_text'], threshold=None, fast_power_method=False) # generate list of ranks for each sentance\n",
    "        rank = pd.DataFrame(i['article_text'], rank) # put each sentance in a data frame with corresponding rank\n",
    "        rank.reset_index(inplace=True)  # reset index\n",
    "        rank['order'] = rank.index  # create column to keep track of the order of each sentance\n",
    "        rank.rename({'index':'rank', 0:'sentance'}, inplace=True, axis=1) # rename columns to make sense\n",
    "        #check length of article and select sentance cuttoff start point\n",
    "        approx_len = sum([len(sentance.split(' ')) for sentance in i['article_text']])\n",
    "        if approx_len == 0:\n",
    "            continue\n",
    "        ratio = 4096 / approx_len  # should be something like 0.5 if its 100% oversized used to approximate split point of data to speed up algorithm\n",
    "        sentance_cuttoff = int(len(rank) * ratio) + 10 # attempt to gather extra to pair down as needed (select 10 extra sentances)\n",
    "        # if the article is too long, slice away sentances until it is short enough\n",
    "        while approx_len > 4096:\n",
    "            temp_article = []\n",
    "            # append sentances from sorted rank data frame tp temp_article until the sentance cuttoff is reached (only the most important sentances remain)\n",
    "            for j in rank.sort_values('rank', ascending=False).sentance[:sentance_cuttoff]:\n",
    "                temp_article.append(j)\n",
    "            # check token length of temp_article to see if we have cutt off enough of the document\n",
    "            approx_len = sum([len(sentance.split(' ')) for sentance in temp_article])\n",
    "            # reduce sentance cuttoff by 1\n",
    "            sentance_cuttoff -=1\n",
    "        # when token lenght is satisfied, add record to final dataframe\n",
    "        rank = rank.sort_values('rank', ascending=False)[:sentance_cuttoff] # select only up to cuttoff point\n",
    "        rank.sort_values('order', ascending=True, inplace=True) # implement temporal correction\n",
    "        # re-assemble truncated text and create a single continuous text field\n",
    "        truncated_text = []\n",
    "        for k in rank.sentance:\n",
    "            truncated_text.append(k)\n",
    "        truncated_text = to_paragraph(truncated_text) # truncated text\n",
    "        summary = to_paragraph(i['abstract_text']) # summary\n",
    "        addition = pd.DataFrame([[truncated_text, summary]],columns=['text', 'target'])\n",
    "        truncated_df = truncated_df.append(addition, ignore_index=True)\n",
    "        #print(len(truncated_df))\n",
    "        if count % 100 == 0 and count !=0:\n",
    "            print(f'processed {count} documents')\n",
    "            print(len(truncated_df))\n",
    "        count +=1\n",
    "    return truncated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c10ac23-6de0-480a-a76e-d4e88715e1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100 documents\n",
      "101\n",
      "processed 200 documents\n",
      "201\n",
      "processed 300 documents\n",
      "301\n",
      "processed 400 documents\n",
      "401\n",
      "processed 500 documents\n",
      "501\n",
      "processed 600 documents\n",
      "601\n",
      "processed 700 documents\n",
      "701\n",
      "processed 800 documents\n",
      "801\n",
      "processed 900 documents\n",
      "901\n",
      "processed 1000 documents\n",
      "1001\n",
      "processed 1100 documents\n",
      "1101\n",
      "processed 1200 documents\n",
      "1201\n",
      "processed 1300 documents\n",
      "1301\n",
      "processed 1400 documents\n",
      "1401\n",
      "processed 1500 documents\n",
      "1501\n",
      "processed 1600 documents\n",
      "1601\n",
      "processed 1700 documents\n",
      "1701\n",
      "processed 1800 documents\n",
      "1801\n",
      "processed 1900 documents\n",
      "1901\n",
      "processed 2000 documents\n",
      "2001\n",
      "processed 2100 documents\n",
      "2101\n",
      "processed 2200 documents\n",
      "2201\n",
      "processed 2300 documents\n",
      "2301\n",
      "processed 2400 documents\n",
      "2401\n",
      "processed 2500 documents\n",
      "2501\n",
      "processed 2600 documents\n",
      "2601\n",
      "processed 2700 documents\n",
      "2701\n",
      "processed 2800 documents\n",
      "2801\n",
      "processed 2900 documents\n",
      "2901\n",
      "processed 3000 documents\n",
      "3001\n",
      "processed 3100 documents\n",
      "3101\n",
      "processed 3200 documents\n",
      "3201\n",
      "processed 3300 documents\n",
      "3301\n",
      "processed 3400 documents\n",
      "3401\n",
      "processed 3500 documents\n",
      "3501\n",
      "processed 3600 documents\n",
      "3601\n",
      "processed 3700 documents\n",
      "3701\n",
      "processed 3800 documents\n",
      "3801\n",
      "processed 3900 documents\n",
      "3901\n",
      "processed 4000 documents\n",
      "4001\n",
      "processed 4100 documents\n",
      "4101\n",
      "processed 4200 documents\n",
      "4201\n",
      "processed 4300 documents\n",
      "4301\n",
      "processed 4400 documents\n",
      "4401\n",
      "processed 4500 documents\n",
      "4501\n",
      "processed 4600 documents\n",
      "4601\n",
      "processed 4700 documents\n",
      "4701\n",
      "processed 4800 documents\n",
      "4801\n",
      "processed 4900 documents\n",
      "4901\n",
      "processed 5000 documents\n",
      "5001\n",
      "processed 5100 documents\n",
      "5101\n",
      "processed 5200 documents\n",
      "5201\n",
      "processed 5300 documents\n",
      "5301\n",
      "processed 5400 documents\n",
      "5401\n",
      "processed 5500 documents\n",
      "5501\n",
      "processed 5600 documents\n",
      "5601\n",
      "processed 5700 documents\n",
      "5701\n",
      "processed 5800 documents\n",
      "5801\n",
      "processed 5900 documents\n",
      "5901\n",
      "processed 6000 documents\n",
      "6001\n",
      "processed 6100 documents\n",
      "6101\n",
      "processed 6200 documents\n",
      "6201\n",
      "processed 6300 documents\n",
      "6301\n",
      "processed 6400 documents\n",
      "6401\n",
      "processed 6500 documents\n",
      "6501\n",
      "processed 6600 documents\n",
      "6601\n"
     ]
    }
   ],
   "source": [
    "# create df and save as csv\n",
    "ranked_val_df = re_order_temporal(val)\n",
    "ranked_val_df.to_csv('ranked_temporal_val_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f112fc0-6320-46d2-90c8-9d1378e3619c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
