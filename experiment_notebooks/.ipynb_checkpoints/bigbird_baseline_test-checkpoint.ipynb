{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e6adea-4020-4bec-a112-175ce3565ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sys\n",
    "import pickle\n",
    "from rouge_score import rouge_scorer\n",
    "from rouge_score import scoring\n",
    "import random\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de75c91b-dc4b-4727-a735-59c1bab26e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# delete research article \n",
    "class ResearchArticle(object):\n",
    "    def __init__(self, article_id, abstract_text, article_text):\n",
    "        \"\"\"\n",
    "          'article_id': str,\n",
    "          'abstract_text': List[str],\n",
    "          'article_text': List[str],\n",
    "        \"\"\"\n",
    "        self.article_id = article_id\n",
    "        self.abstract_text = abstract_text\n",
    "        self.article_text = article_text\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    path - path where data is stored\n",
    "\n",
    "    data - returns data as ResearchArticle objects\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='bytes')\n",
    "    return data\n",
    "\n",
    "def shuffle_and_sample(data):\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "    return data[:600]\n",
    "\n",
    "\n",
    "def to_paragraph(text):\n",
    "    \"\"\"\n",
    "    converts text comprised of lists of sentances to a single to_paragraph\n",
    "\n",
    "    text - list of multiple string sentances\n",
    "\n",
    "    continuous_text - single continuous text string\n",
    "    \"\"\"\n",
    "    continuous_text = ''\n",
    "    for line in text:\n",
    "        continuous_text += line\n",
    "    return continuous_text\n",
    "\n",
    "def get_scores(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    if hypothesis and or reference is converted to summary or not, this returns\n",
    "    the rouge score of the two\n",
    "\n",
    "    hypothesis - list of sentances or single paragraph\n",
    "\n",
    "    reference - list of sentances or single paragraph\n",
    "\n",
    "    score - rouge scores Todo:specify output\n",
    "    \"\"\"\n",
    "    if len(hypothesis) !=1:\n",
    "        hypothesis_abstract = to_paragraph(hypothesis)\n",
    "    if len(reference) != 1:\n",
    "        reference_abstract = to_paragraph(reference)\n",
    "    rouge = Rouge()\n",
    "    score = rouge.get_scores(hypothesis_abstract, reference_abstract)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969b6f3-133b-47b9-af94-80db711b9975",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "954c6552-0dc7-4b28-944c-ecaa46b51f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/mini_val_set_json.txt') as json_file:\n",
    "    mini_val = json.load(json_file)\n",
    "mini_val = mini_val['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce70deb0-9a74-4150-8553-90721c736840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: delete\n",
    "validation_data = load_data('pudmed_val.pk.bin')\n",
    "mini_val_data = shuffle_and_sample(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094785f-e8fa-46e4-b7e2-3a30e012a1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 documents\n",
      "Processed 10 documents\n"
     ]
    }
   ],
   "source": [
    "summaries_df = pd.DataFrame(columns=['reference', 'prediction'])\n",
    "results = {}\n",
    "# initialize tokenizer, model, and scorer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-pubmed\")\n",
    "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-pubmed\")\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeLsum\"], use_stemmer=True)\n",
    "aggregator = scoring.BootstrapAggregator()\n",
    "for i in range(len(mini_val)):\n",
    "    input = tokenizer(mini_val[i]['article_text'], is_split_into_words=True, return_tensors='pt', max_length=4096, truncation=True)\n",
    "    prediction = model.generate(**input)\n",
    "    prediction = tokenizer.batch_decode(prediction)\n",
    "    score = scorer.score(to_paragraph(mini_val[i]['abstract_text']), prediction[0])\n",
    "    aggregator.add_scores(score)\n",
    "    ag_score = aggregator.aggregate()\n",
    "    results[mini_val[i]['article_id']] = score\n",
    "    addition = pd.DataFrame([[to_paragraph(mini_val[i]['abstract_text']), prediction[0]]],columns=['reference', 'prediction'])\n",
    "    summaries_df = summaries_df.append(addition, ignore_index=True)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Processed {i} documents')\n",
    "final_ag_score = ag_score\n",
    "print('Completed document evaluation')\n",
    "print(final_ag_score)\n",
    "with open(\"trucated_doc_results.txt\", 'w') as outfile:\n",
    "    json.dump(results, outfile)\n",
    "summaries_df.to_csv('bigbird_baseline_summaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e09cab-7705-4147-88aa-9b5c33c6922d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 papers\n",
      "Processed 20 papers\n",
      "Processed 30 papers\n",
      "Processed 40 papers\n",
      "Processed 50 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 458 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60 papers\n",
      "Processed 70 papers\n",
      "Processed 80 papers\n",
      "Processed 90 papers\n",
      "Processed 100 papers\n",
      "Processed 110 papers\n",
      "Processed 120 papers\n",
      "Processed 130 papers\n",
      "Processed 140 papers\n",
      "Processed 150 papers\n",
      "Processed 160 papers\n",
      "Processed 170 papers\n",
      "Processed 180 papers\n",
      "Processed 190 papers\n",
      "Processed 200 papers\n",
      "Processed 210 papers\n",
      "Processed 220 papers\n",
      "Processed 230 papers\n",
      "Processed 240 papers\n",
      "Processed 250 papers\n",
      "Processed 260 papers\n",
      "Processed 270 papers\n",
      "Processed 280 papers\n",
      "Processed 290 papers\n",
      "Processed 300 papers\n",
      "Processed 310 papers\n",
      "Processed 320 papers\n",
      "Processed 330 papers\n",
      "Processed 340 papers\n",
      "Processed 350 papers\n",
      "Processed 360 papers\n",
      "Processed 370 papers\n",
      "Processed 380 papers\n",
      "Processed 390 papers\n",
      "Processed 400 papers\n",
      "Processed 410 papers\n",
      "Processed 420 papers\n",
      "Processed 430 papers\n",
      "Processed 440 papers\n",
      "Processed 450 papers\n",
      "Processed 460 papers\n",
      "Processed 470 papers\n",
      "Processed 480 papers\n",
      "Processed 490 papers\n",
      "Processed 500 papers\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "validation_data = load_data('pudmed_val.pk.bin')\n",
    "mini_val_data = shuffle_and_sample(validation_data)\n",
    "#print('loaded data!')\n",
    "# iterate over dev set\n",
    "total_papers = 0\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeLsum\"], use_stemmer=True) # todo: evaluate if this when changed to l(not sum) is the same\n",
    "aggregator = scoring.BootstrapAggregator()\n",
    "#print('initialized score package!')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-pubmed\")\n",
    "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-pubmed\")\n",
    "#print('loaded model and tokenizer!')\n",
    "#max_len = 4096  # maximum length the model can take in at once\n",
    "i = 0\n",
    "for paper in mini_val_data:\n",
    "    #paper_abstract = to_paragraph(paper.abstract_text)  # create a single text line\n",
    "    input = tokenizer(paper.article_text, return_tensors='pt', is_split_into_words=True, max_length=4096, truncation=True)\n",
    "    #print('tokenized input')\n",
    "    prediction = model.generate(**input)\n",
    "    #print('generated prediction')\n",
    "    prediction = tokenizer.batch_decode(prediction)\n",
    "    #print('decoded predictin')\n",
    "    #print(prediction[0])\n",
    "    # evaluate summary compared to ground truth\n",
    "    total_papers += 1\n",
    "    reference = to_paragraph(paper.abstract_text)\n",
    "    #print('generated reference')\n",
    "    score = scorer.score(reference, prediction[0])\n",
    "    #print('scored prediction')\n",
    "    aggregator.add_scores(score)\n",
    "    #print('aggregated score')\n",
    "    ag_score = aggregator.aggregate()\n",
    "    file1 = open(\"mini_val_records.txt\",\"a\")\n",
    "    file1.write(str({'index': i, 'article_id': {paper.article_id}, 'predicted_summary': prediction[0], 'ground_truth': reference, 'score': score ,'aggregate_score': ag_score}))\n",
    "    file1.close()\n",
    "    if total_papers % 10 == 0:\n",
    "        print(f'Processed {total_papers} papers')\n",
    "        # todo: run again and save score, as well as the file name\n",
    "    i += 1\n",
    "#print final output\n",
    "file2 = open(\"final_record.txt\",\"a\")\n",
    "file2.write(str({'mini_val_data_aggregate_score': ag_score}))\n",
    "file2.close()\n",
    "print(f'Aggregate ROUGE Scores:\\n {ag_score}')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
